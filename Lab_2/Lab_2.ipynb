{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d8f08d9",
   "metadata": {},
   "source": [
    "# 2023/2024 - Ma412 \n",
    "\n",
    "# Lab 2: Sparse Regression "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d071710",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to use different linear regression algorithms on the dataset Boston House Price. This will include becoming familiar with the notions of regularization and selection of variables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "044d32e2",
   "metadata": {},
   "source": [
    "## 1. Linear regression\n",
    "\n",
    "We consider $X$ $\\in\\mathbb{R}^{l\\times(n+1)}$ the matrix containing the data whose the ith row is $(x_i,1)$ and $Y$ $\\in\\mathbb{R}^{l}$ the vector containing the labels $y_i$. We consider the least squares estimator the vector $$\n",
    "\\begin{pmatrix}\n",
    "\\hat{\\alpha}\\\\[3mm]\n",
    "\\hat{\\beta} \\\\[3mm]\n",
    "\\end{pmatrix}= (X^TX)^{-1}X^TY=min_{\\alpha\\in\\mathbb{R}^{n}, \\beta\\in\\mathbb{R}}\\sum_{i=1}^l(y_i-(<\\alpha,x_i>+\\beta))^2$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad963b50",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "1. Program a $regression(X, Y)$ function that returns the least squares estimator. Use your regression function on the Boston House Prices dataset (to be loaded with\n",
    "the $datasets.load\\_boston()$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1af1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "def regression(X,Y):\n",
    "    teta =  np.linalg.lstsq(X.T@X,X.T@Y)\n",
    "    return teta[0:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7846bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n",
      "36.459488385075296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CE PC\\AppData\\Local\\Temp\\ipykernel_16560\\407321962.py:6: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  teta =  np.linalg.lstsq(X.T@X,X.T@Y)[0]\n",
      "C:\\Users\\CE PC\\AppData\\Local\\Temp\\ipykernel_16560\\407321962.py:7: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  res =  np.linalg.lstsq(X.T@X,X.T@Y)[1]\n"
     ]
    }
   ],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "xi = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "yi = raw_df.values[1::2, 2]\n",
    "X = np.hstack([xi,np.ones((xi.shape[0],1))])\n",
    "Y = yi\n",
    "\n",
    "teta,residuals = regression(X,Y)\n",
    "alpha = teta[:-1]\n",
    "beta = teta[-1]\n",
    "print(alpha)\n",
    "print(beta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0d0a310",
   "metadata": {},
   "source": [
    "2. Compare the vectors $\\hat{\\alpha}$ and $\\hat{\\beta}$ returned by the function by using with the $coef\\_$ and $intercept\\_$ attributes of a $linear\\_model.LinearRegression$.\n",
    "Some useful functions: $dot()$, $transpose()$, $pinv()$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e147f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n",
      "[-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
      " -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
      "  3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
      " -5.24758378e-01]\n",
      "36.45948838508985\n",
      "36.459488385075296\n",
      "R² linear :  0.7406426641094095\n",
      "R² linear :  []\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(xi, yi)\n",
    "print(reg.coef_)\n",
    "print(alpha)\n",
    "print(reg.intercept_)\n",
    "print(beta)\n",
    "print(\"R² linear : \",reg.score(xi,yi))\n",
    "print(\"R² linear : \", residuals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14b406a7",
   "metadata": {},
   "source": [
    "3. Write the funcion $regress(X,\\alpha, \\beta)$ which returns the vector $\\hat{Y}$ of the predicted labels\n",
    "such as $\\hat{y}_i=<\\alpha,x_i>+\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43298b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11078.784577954975\n"
     ]
    }
   ],
   "source": [
    "def regress(X,alpha,beta):\n",
    "    Y = np.zeros((X.shape[0]))\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y[i] = alpha @ X[i] + beta\n",
    "    return Y\n",
    "Y_lst = regress(xi,alpha,beta)\n",
    "err = np.linalg.norm(Y-Y_lst,2)**2\n",
    "print(err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5451a897",
   "metadata": {},
   "source": [
    "4. Calculate the least squares error $\\epsilon=\\lVert Y-\\hat{Y} \\rVert_2^2=\\sum_{i=1}^l(y_i-\\hat{y}_i)^2$ of the learned regressor about the entire Boston dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1f46794",
   "metadata": {},
   "source": [
    "## 2. Ridge regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad51dd59",
   "metadata": {},
   "source": [
    "In some cases, the matrix $X^TX$ is not invertible. To remedy to this problem, we add a ridge $\\lambda\\mathbb{1}$ to this matrix where $\\mathbb{1}\\in\\mathbb{R}^{(n+1)\\times(n+1)}$ is the following matrix:\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "1 & ... & \\cdots & 0 \\\\\n",
    "0& \\ddots & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & 1 & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 0\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f01e0aa",
   "metadata": {},
   "source": [
    "This corresponds to a slight modification of the optimization problem which penalizes the size of the\n",
    "coefficients. The generalized least squares vector is given by: \n",
    "$$\\begin{pmatrix}\n",
    "\\hat{\\alpha}\\\\[3mm]\n",
    "\\hat{\\beta} \\\\[3mm]\n",
    "\\end{pmatrix}= (X^TX+\\lambda\\mathbb{1})^{-1}X^TY=min_{\\alpha\\in\\mathbb{R}^{n}, \\beta\\in\\mathbb{R}}\\sum_{i=1}^l(y_i-(<\\alpha,x_i>+\\beta))^2+\\lambda\\lVert \\alpha \\rVert_2^2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "599a8e8c",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "1. Program a $ridge\\_regression(X, Y,\\lambda)$ function that returns the least squares estimator. Compare again the vectors $\\hat{\\alpha}$ and $\\hat{\\beta}$ obtained for the parameter\n",
    "$\\lambda = 1$ on the Boston dataset using $coe\\_$ and $intercept\\_$ attributes of a regressor $linear\\_model.Ridge$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Your code here ...\n",
    "# ============================================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f96d1c2d",
   "metadata": {},
   "source": [
    "2. Plot the evolution of the coefficients of the vector $\\hat{\\alpha}$ as a function of the regularization parameter\n",
    "$\\lambda$ for values between $1e-3$ and $1e3$. Which variables seem to best explain the house prices in Boston?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ============================================================\n",
    "# Your code here ...\n",
    "# ============================================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8b69a8a",
   "metadata": {},
   "source": [
    "3. Find by some appropriate means the best value for the parameter $\\lambda$ . Learn then and run a regressor with this value on the entire Boston dataset and compute\n",
    "the error in the sense of least squares on this same sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33282153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Your code here ...\n",
    "# ============================================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26e46b0b",
   "metadata": {},
   "source": [
    "## 3. LASSO regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83e6b8d8",
   "metadata": {},
   "source": [
    "In this regularization, the penalization of the vector of the coefficients is done here with the norm $l_1$ instead of the Euclidean norm $l_2$. Consider $\\alpha\\in\\mathbb{R}^{n}$, $\\lVert \\alpha \\rVert_1=\\sum_{i=1}^n|\\alpha_i|$. Solutions are then parsimonious. The optimization problem is given by:\n",
    "$$min_{\\alpha\\in\\mathbb{R}^{n}, \\beta\\in\\mathbb{R}}\\sum_{i=1}^l(y_i-(<\\alpha,x_i>+\\beta))^2+\\lambda\\lVert \\alpha \\rVert_1$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d446e2cf",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "1. Using the $linear\\_model.Lasso$ class, plot the evolution of the coefficients of the vector $\\hat{\\alpha}$ regarding the value of the parameter $\\lambda$. Which variables seem to best explain the\n",
    "house prices in Boston? Are they the same as those found in the previous exercise? How do other variables behave when the value of $\\lambda$ increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b28364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Your code here ...\n",
    "# ============================================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "097bce41",
   "metadata": {},
   "source": [
    "2. Find by some appropriate means the best value for the $\\lambda$ parameter. Learn then and run a regressor with this value on the entire Boston dataset and compute\n",
    "the error in the sense of least squares on this same sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f652d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Your code here ...\n",
    "# ============================================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e5eddcc",
   "metadata": {},
   "source": [
    "## 4. Elastic Net Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be093f1e",
   "metadata": {},
   "source": [
    "In Elastic Net Regularization we add the both terms of $l_1$ and $l_2$ to get the final loss function. Referring to the course, apply the elastic Net regularization to train your model, calculate the prediction and the mean square error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d02a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Your code here ...\n",
    "# ============================================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdb3c172",
   "metadata": {},
   "source": [
    "From the above analysis, which conclusions can you reach about the different regularization methods ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aea6fe81",
   "metadata": {},
   "source": [
    "## 5. Your Turn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07b8659d",
   "metadata": {},
   "source": [
    "The purpose now is to test these approaches on other datasets. You may choose one from the UCI machine learning repository http://archive.ics.uci.edu/.\n",
    "Download a dataset, and try to determine the optimal set of parameters to use to model it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9124e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Your code here ...\n",
    "# ============================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
